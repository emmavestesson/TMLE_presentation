---
title: "Causal inference in R - a practical introduction to the tlverse"
format: 
  revealjs:
    slide-number: c/t 
    theme: moon
editor: visual
---

## About me

-   Principal data analyst at the Health Foundation
-   Part-time PhD student at the Institute of Child Health at UCL
-   R-Ladies London organiser
-   #rdogladies

## Content

-   My origin story
-   A brief theoretical introduction to causal inference
-   Theory of TMLE
-   Finally some code! (`sl3` and `tmle3`)

## My motivation for using causal inference

-   What is the effect on antibiotic prescribing of having a remote GP consultation compared to a face-to-face consultation?

-   Important questions as the pandemic has increased the use of remote consultations in general practice

-   We need to preserve antibiotics to minimise antibiotic resistance

## 

::: {layout-ncol="2"}
![](https://images.unsplash.com/photo-1592422207043-9e19520a698e?ixlib=rb-4.0.3&ixid=MnwxMjA3fDB8MHxzZWFyY2h8MTJ8fHlvdW5nJTIwcGhvbmV8ZW58MHx8MHx8&auto=format&fit=crop&w=500&q=60)

![](https://images.unsplash.com/photo-1498757581981-8ddb3c0b9b07?ixlib=rb-4.0.3&ixid=MnwxMjA3fDB8MHxwaG90by1wYWdlfHx8fGVufDB8fHx8&auto=format&fit=crop&w=688&q=80)
:::

## About this topic

-   Machine learning has been all the rave - mainly focussed on prediction
-   Causal inference is used to determe the independent, actual effect of a treatment on a given outcome

![](https://imgs.xkcd.com/comics/correlation.png)

# Different types of causal inference

-   Interrupted timeseries

-   Difference-in-difference

-   Regression discontinuity design

-   Instrumental variables

-   Matching `matchIt`

-   (Augmented) Inverse probability weighting `AIPW`

-   Targeted maximum likelihood estimation `tmle3`

```{r}
library(tidyverse)
library(gt)

Q <- readRDS(here::here('data', 'Q.RDS'))
W_A <- readRDS(here::here('data', 'W_A.RDS'))
dat_obs <- readRDS( here::here('data', 'dat_obs.RDS'))

Q <- readRDS(here::here('data', 'Q.RDS'))
g <- readRDS(here::here('data', 'g.RDS'))


```

## Targeted maximum likelihood (TMLE)

TMLE has two main parts

-   Estimate the conditional probability of being exposed based on confounders that you have identified
-   Estimate the expected value of the outcome using treatment and confounders as predictors the outcome
-   Combines that to estimate the average treatment effect

It is doubly robust so if you get either of those two right then your estimate is consistent

## Causal assumptions

It is not enough to use a method that is suitable for causal inference to make causal claims

::: incremental
1.  The stable treatment assumption the treatment status of any individual does not affect the potential outcomes of other individuals

2.  No unmeasured confounding. This means that all common causes of both the treatment and the outcome have been measured.

3.  Positivity. Within each strata of the set of covariates each individual has a nonzero probability of receiving either treatment.
:::

# 

![](tlverse_github.png)

## Toy example

Basic version of the TMLE algorithm - estimating the mean difference in outcomes, adjusted for confounders, for a binary outcome and binary treatment.

If causal assumptions are met, this is called the Average Treatment Effect (ATE), or the mean difference in outcomes in a world in which everyone had received the treatment compared to a world in which everyone had not.

## Step 1: estimate the expected value of the outcome

```{r}
#| include: true
#| eval: false
#| echo: true
Q <- glm(Y_abx ~ A_remote + covar1 + covar2, family = 'binomial')
```

## 

We then use that model to predict what the estimated outcome would be if

::: panel-tabset
## Actual treatment

```{r}
#| include: true
#| eval: false
#| echo: true
Q_A <- predict(Q)$pred %>% 
  as.vector()# predictions using the treatment they actually received

```

## Everyone got the treatment

```{r}
#| include: true
#| echo: true
#| eval: false
W_A1 <- W_A %>%
  mutate(A_remote = 1)  # data set where everyone received treatment
Q_1 <- predict(Q, newdata = W_A1)$pred %>% 
  as.vector()# predict on that everyone-exposed data set

```

## Everyone got the control

```{r}
#| include: true
#| echo: true
#| eval: false

W_A0 <- W_A %>% 
  mutate(A_remote = 0) # data set where no one received treatment
Q_0 <- predict(Q, newdata = W_A0)$pred %>% 
  as.vector()
```
:::

## Combined data

```{r}
#| include: true
#| echo: false
#| eval: true
dat_tmle <- readRDS(here::here('data', 'dat_tmle.RDS'))
dat_tmle %>%
  slice_head(n = 5) %>%  
    select(-c(H_1, H_0)) %>% 
  gt() %>% 
  fmt_number(columns = c(Q_A, Q_0, Q_1)) %>% 
  tab_style(
    style = cell_fill(color = "teal"),
    locations = cells_body(
      columns =Q_A,
    )) %>% 
  tab_style(
    style = cell_fill(color = "teal"),
    locations = cells_body(
      columns =Q_0,
      rows = A_remote == 0
    )) %>% 
  tab_style(
    style = cell_fill(color = "teal"),
    locations = cells_body(
      columns =Q_1,
      rows = A_remote == 1
    )) 

```

## g-computation ATE

```{r}
#| include: true
#| eval: false
#| echo: true
mean(dat_tmle$Q_1-dat_tmle$Q_0)
```

Does not fully account for the differences between the individuals getting the treatment and those that didn't

## Step 2: Estimate the probability of treatment

We want to estimate the probability of treatment (ie the propensity score).

```{r}
#| include: true
#| eval: false
#| echo: true
g <- glm(A_remote ~  covar1 + covar2, family = 'binomial')
```

## Calculate a clever covariate

We use `g` to predict values and use the results to calculate the 'clever covariate'

1.  The inverse probability of receiving treatment.
2.  The negative inverse probability of not receiving treatment.
3.  If the observation was treated, the inverse probability of receiving treatment, and if they were not treated, the negative inverse probability of not receiving treatment. (This is the clever covariate, `H_A`)

## 

```{r}
#| include: true
#| echo: true
#| eval: false
g_w <- predict(g)$pred %>% 
  as.vector() # Pr(A=1|W)
H_1 <- 1/g_w
H_0 <- -1/(1-g_w) # Pr(A=0|W) is 1-Pr(A=1|W)
dat_tmle <- # add clever covariate data to dat_tmle
  dat_tmle %>%
  bind_cols(
    H_1 = H_1,
    H_0 = H_0) %>%
  mutate(H_A = case_when(A_remote == 1 ~ H_1, # if A is 1 (treated), assign H_1
                         A_remote == 0 ~ H_0))  # if A is 0 (not treated), assign H_0

```

## Step 3: Estimate the Fluctuation Parameter

The point of this step is to solve an estimating equation for the efficient influence function (EIF) of our estimand of interest.

Logit(True outcome) = logit(initial outcome estimate) + eps\*H_A

We can estimate eps using a logistic regression with one covariate (the clever covariate), no intercept and the initial outcome estimate as a fixed intercept (offset)

```{r}
#| include: true
#| eval: false
#| echo: true


glm(Y_abx ~ -1 + offset(qlogis(Q_A)) + H_A, data=df , family='binomial')
```

## 

::: incremental
Step 2 is kind of equivalent to regressing the residuals from the initial outcome regression onto a covariate that is a function of the propensity score

This means that if the initial model in step 1 was correctly specified then there is no signal in the residuals → epsilon will be small

Strong residual confounding → large epsilon → large update Weak residual confounding → small epsilon → small update
:::

## Quick recap

::: incremental
In Step 1, we obtained initial estimates of the expected outcome. These estimates are optimized to estimate E\[Y\|A,W\] (expected value of the outcome) not the ATE.

We need to update those initial expected outcome estimates using information about the treatment mechanism, so we computed the expected probability of treatment, conditional on confounders, in Step 2.

Step 3, we used quantities from Step 1 and Step 2 to solve an estimating equation for the EIF. This gave us an estimate of epsilon.
:::

## Step 4: update the initial estimates of the expected outcome

We are updating the estimates from step 1 using information from step 2 and 3. The size of epsilon determines the size of the update.

```{r}
#| include: true
#| eval: false
#| echo: true

Q_A_update <- plogis(qlogis(Q_A) + eps*H_A) # updated expected outcome given treatment actually received
Q_1_update <- plogis(qlogis(Q_1) + eps*H_1) # updated expected outcome for everyone receiving treatment
Q_0_update <- plogis(qlogis(Q_0) + eps*H_0) # updated expected outcome for everyone not receiving treatment
```

## 

1\. Update the expected outcomes of all observations, given the treatment they actually received and their baseline confounders. `Q_A_updated`

2\. Update the expected outcomes, conditional on baseline confounders and everyone receiving the treatment. (`Q_1_updated`)

3\. Update the expected outcomes, conditional on baseline confounders and no one receiving the treatment. (`Q_0_updated`)

## Step 5: compute the ATE

Mean difference between the updated outcome estimates when everyone is treated vs when no one is treated

ATE = mean(Q_1\_updated - Q_0\_updated)

## Steps we will ignore

The standard errors

##  {background-image="https://images.unsplash.com/photo-1591453089816-0fbb971b454c?ixlib=rb-4.0.3&ixid=MnwxMjA3fDB8MHxwaG90by1wYWdlfHx8fGVufDB8fHx8&auto=format&fit=crop&w=2940&q=80"}

## Super learner

An ensemble machine learning that combines multiple learners (algorithms) and uses cross-validation to select the optimal ensemble (collection) of learners making a single new prediction algorithm This can either be one learner or a combination of the different learners

[![](https://koalaverse.github.io/machine-learning-in-R/images/h2oEnsemble.png){width="16cm"}](https://koalaverse.github.io/machine-learning-in-R/images/h2oEnsemble.png)

## Cross-validation

Create a cross validation set of your data (eg 5 folds)

Train all the models on 4 out of the 5 data chunks using the fifth chunk to get predictions

Repeat this until each chunk has been left out one time and you have a set of cross-validated predictions for the full data set for each model

## 

pred_mod_a -- predictions from the cross validation from model a

pred_mod_b -- predictions from the cross validation from model b

pred_mod_c -- predictions from the cross validation from model c

pred_mod_d -- predictions from the cross validation from model d

These are our cross validated predictions

Predictions on based data that was not used to train the model

## Predict the outcome using the cross-validated predictions

Use the cross-validated predictions to predict the outcome

Y \~ a*pred_mod_a + b*pred_mod_b + c*pred_mod_c + d*pred_mod_d

The model used is called a metalearner and the actual model can be anything! (default is a Non-Negative Least Squares)

## 

Learners that did well will have a larger coefficient and therefore have more of a weight

Y \~ 2*pred_mod_a + 0.5*pred_mod_b + 9*pred_mod_c + 3*pred_mod_d

This is our super learner algorithm

## 

Fit base learners (our 4 models) on the entire data set and get predictions. This is your new dataset.

Plug that data into the super learner to get final predictions!

The `sl3` package will do all of this for you

## Picking your learners

The learners should have a diverse set of learning strategies

Pick learners based on

::: incremental

-   Data types (eg add models that handle splines if you have continuous data)

-   Number of predictors (if many then use learners that reduce the dimensionality)

-   Number of learners depends on how good your computer is
::: 

## `sl3` learners

```{r}
library(sl3)
```


```{r}
#| include: true
#| eval: true
#| echo: true

sl3_list_properties()
```

## 

```{r}
#| include: true
#| eval: true
#| echo: true

sl3_list_learners(properties = 'binomial')
```

## Specify learners

```{r}
#| include: true
#| eval: true
#| echo: true

glm_learner <- Lrnr_glm$new(family = "binomial")
lasso_learner <- Lrnr_glmnet$new(family = "binomial")
ridge_learner <- Lrnr_glmnet$new(alpha = 0, family = "binomial")

ls_metalearner <- make_learner(Lrnr_nnls)

```

## Run TMLE

```{r}
#| include: true
#| eval: false
#| echo: true


sl_Y <- Lrnr_sl$new(
  learners = list(glm_learner, lasso_learner, ridge_learner),
  metalearner = ls_metalearner
)
sl_A <- Lrnr_sl$new(
  learners = list(glm_learner, lasso_learner, ridge_learner),
  metalearner = ls_metalearner
)
learner_list <- list(A = sl_A, Y = sl_Y)

tmle_fit <- tmle3(ate_spec, washb_data, node_list, learner_list)

```

## Combine learners

```{r}
#| include: true
#| eval: true
#| echo: true

my_stack <- Stack$new(glm_learner, lasso_learner, ridge_learner)
my_stack
```

## Define and train the super learner

```{r}
#| include: true
#| eval: false
#| echo: true

sl <- Lrnr_sl$new(learners = stack, metalearner = Lrnr_nnls$new())
sl_fit <- sl$train(task = task)
sl_preds <- sl_fit$predict(task = task)
```

## 

## Keen to learn more?

TMLE

-   https://www.khstats.com/blog/tmle/tutorial.html
-   https://www.youtube.com/watch?v=8Q9dfW3oOi4&list=PLy_CaFomwGGGH10tbq9zSyfHVrdklMaLe&index=2

Super learner

-   https://youtu.be/1zT17HtvtF8
-   https://www.youtube.com/watch?v=WYnjja8DKPg
-   https://ui.adsabs.harvard.edu/abs/2022arXiv220406139P/abstract
