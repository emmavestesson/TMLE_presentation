---
title: "Causal inference in R - a practical introduction to the tlverse"
format: revealjs
editor: visual
---

## About me

-   Principal data analyst at the Health Foundation
-   Part-time PhD student at the Institute of Child Health at UCL
-   R-Ladies London organiser
-   #rdogladies

## My motivation for using causal inference

-   What is the effect on antibiotic prescribing of having a remote GP consultation compared to a face-to-face consultation?

-   Important questions as the pandemic has increased the use of remote consultations in general practice

-   We need to preserve antibiotics to minimise antibiotic resistance

-   Patients have remote consultations might be different to those that have face-to-face consultations

# Different types of causal inference

-   Interrupted timeseries

-   Difference-in-difference

-   Regression discontinuity design

-   Instrumental variables

-   Matching `matchIt`

-   (Augmented) Inverse probability weighting `AIPW`

-   Targeted maximum likelihood estimation `tmle3`

## About this topic

-   Machine learning has been all the rave - mainly focussed on prediction
-   Causal inference is the process of determining the independent, actual effect of a particular phenomenon that is a component of a larger system

![](https://imgs.xkcd.com/comics/correlation.png)

## 

```{r}
library(tidyverse)
library(gt)

Q <- readRDS(here::here('data', 'Q.RDS'))
W_A <- readRDS(here::here('data', 'W_A.RDS'))
dat_obs <- readRDS( here::here('data', 'dat_obs.RDS'))

Q <- readRDS(here::here('data', 'Q.RDS'))
g <- readRDS(here::here('data', 'g.RDS'))


```

## Targeted maximum likelihood (TMLE)

TMLE has two main parts

-   Estimate the conditional probability of being exposed based on confounders that you have identified
-   Estimate the expected value of the outcome using treatment and confounders as predictors the outcome
-   Combines that to estimate the average treatment effect

It is doubly robust so if you get either of those two right then your estimate is consistent

## Causal assumptions

It is not enough to use a method that is suitable for causal inference to make causal claims

1.  The stable treatment assumption the treatment status of any individual does not affect the potential outcomes of other individuals

2.  No unmeasured confounding. This means that all common causes of both the treatment and the outcome have been measured.

3.  Positivity. Within each strata of the set of covariates each individual has a nonzero probability of receiving either treatment.

# 

![](tlverse_github.png)

## ghj

I'll walk step-by-step through a basic version of the TMLE algorithm - estimating the mean difference in outcomes, adjusted for confounders, for a binary outcome and binary treatment.

If causal assumptions are met, this is called the Average Treatment Effect (ATE), or the mean difference in outcomes in a world in which everyone had received the treatment compared to a world in which everyone had not.

## Super learner

An ensemble machine learning that combines multiple learners (algorithms) and uses cross-validation to select the optimal ensemble (collection) of learners making a single new prediction algorithm This can either be one learner or a combination of the different learners

[![](https://koalaverse.github.io/machine-learning-in-R/images/h2oEnsemble.png){width="16cm"}](https://koalaverse.github.io/machine-learning-in-R/images/h2oEnsemble.png)

## Picking your learners

The learners should have a diverse set of learning strategies

Pick learners based on

-   Data types (eg add models that handle splines if you have continuous data)

-   Number of predictors (if many then use learners that reduce the dimensionality)

-   Number of learners depends on how good your computer is

Take a set of models 4 models logistic, random forest, xgboost, polyspline

## Cross-validation

Create a cross validation set of your data (eg 5 folds)

Train all the models on 4 out of the 5 data chunks using the fifth chunk to get predictions

Repeat this until each chunk has been left out one time and you have a set of cross-validated predictions for the full data set for each model

## 

pred_mod_a -- predictions from the cross validation from model a pred_mod_b -- predictions from the cross validation from model b pred_mod_c -- predictions from the cross validation from model c pred_mod_d -- predictions from the cross validation from model d

These are our cross validated predictions

Predictions on based data that was not used to train the model

## Predict the outcome using the cross-validated predictions

Use the cross-validated predictions to predict the outcome

Y \~ a*pred_mod_a + b*pred_mod_b + c*pred_mod_c + d*pred_mod_d

The model used is called a metalearner and the actual model can be anything! (default is a Non-Negative Least Squares)

## 

Learners that did well will have a larger coefficient and therefore have more of a weight

Y \~ 2*pred_mod_a + 0.5*pred_mod_b + 9*pred_mod_c + 3*pred_mod_d

This is our super learner algorithm

## 

Fit base learners (our 4 models) on the entire data set and get predictions. This is your new dataset.

Plug that data into the super learner to get final predictions!

The `sl3` package will do all of this for you

## Step 1: estimate the expected value of the outcome

There is a function Q which takes A (treatment) and W (covariates) as inputs and return the conditional expectation of Y (outcome).

We want to model the outcome (Y_abx). In the TMLE literature this is referred to Q, a function that takes A_remote and W (covariates) as inputs. We estimate Q using superlearner.

```{r, eval=FALSE}
Q <- SuperLearner(Y = Y, # Y is the outcome vector
                  X = W_A, # W_A is the matrix of age, asthma, male,  and A
                  family=binomial(), # specify we have a binary outcome
                  SL.library = sl_libs) 

```

## 

We then use that model to predict what the estimated outcome would be if

::: panel-tabset
## Actual treatment

```{r}
#| include: true
#| eval: false
#| echo: true
Q_A <- predict(Q)$pred %>% 
  as.vector()# predictions using the treatment they actually received

```

## Everyone got the treatment

```{r}
#| include: true
#| echo: true
#| eval: false
W_A1 <- W_A %>%
  mutate(A_remote = 1)  # data set where everyone received treatment
Q_1 <- predict(Q, newdata = W_A1)$pred %>% 
  as.vector()# predict on that everyone-exposed data set

```

## Everyone got the control

```{r}
#| include: true
#| echo: true
#| eval: false

W_A0 <- W_A %>% 
  mutate(A_remote = 0) # data set where no one received treatment
Q_0 <- predict(Q, newdata = W_A0)$pred %>% 
  as.vector()
```
:::

## Combined data

```{r}
#| include: true
#| echo: false
#| eval: true
dat_tmle <- readRDS(here::here('data', 'dat_tmle.RDS'))
dat_tmle %>%
  slice_head(n = 5) %>%  
  gt() %>% 
  fmt_number(columns = c(Q_A, Q_0, Q_1)) %>% 
  tab_style(
    style = cell_fill(color = "teal"),
    locations = cells_body(
      columns =Q_A,
    )) %>% 
  tab_style(
    style = cell_fill(color = "teal"),
    locations = cells_body(
      columns =Q_0,
      rows = A_remote == 0
    )) %>% 
  tab_style(
    style = cell_fill(color = "teal"),
    locations = cells_body(
      columns =Q_1,
      rows = A_remote == 1
    )) 

```

## Step 2: Estimate the probability of treatment

We want to estimate the probability of treatment (ie the propensity score).

```{r, eval=FALSE}
A <- dat_obs$A_remote
W <- dat_obs %>% 
  select(-Y_abx, -A_remote) # matrix of predictors that only contains the confounders age, asthma and male
g <- SuperLearner(Y = A, # outcome is the A (treatment) vector
                  X = W, # W is a matrix of predictors
                  family=binomial(), # treatment is a binomial outcome
                  SL.library=sl_libs) # using same candidate learners; could use different learners

```

## 

We use super learner again and use the results to calculate the 'clever covariate'

1.  The inverse probability of receiving treatment.
2.  The negative inverse probability of not receiving treatment.
3.  If the observation was treated, the inverse probability of receiving treatment, and if they were not treated, the negative inverse probability of not receiving treatment. (This is the clever covariate, `H_A`)

```{r}
#| include: true
#| echo: true
#| eval: false
g_w <- predict(g)$pred %>% 
  as.vector() # Pr(A=1|W)
H_1 <- 1/g_w
H_0 <- -1/(1-g_w) # Pr(A=0|W) is 1-Pr(A=1|W)
dat_tmle <- # add clever covariate data to dat_tmle
  dat_tmle %>%
  bind_cols(
    H_1 = H_1,
    H_0 = H_0) %>%
  mutate(H_A = case_when(A_remote == 1 ~ H_1, # if A is 1 (treated), assign H_1
                         A_remote == 0 ~ H_0))  # if A is 0 (not treated), assign H_0

```

## Step 3: Estimate the Fluctuation Parameter

The point of this step is to solve an estimating equation for the efficient influence function (EIF) of our estimand of interest.

Logit(True outcome) = logit(initial outcome estimate) + eps\*clever covariate (H_A)

We can estimate eps using a logistic regression with one covariate (the clever covariate), no intercept and the initial outcome estimate as a fixed intercept (offset)

glm(Y \~ -1 + offset(qlogis(Q_A)) + H_A, data=df , family=binomial)

## 

Step 2 is kind of equivalent to regressing the residuals from the initial outcome regression onto a covariate that is a function of the propensity score

This means that if the initial model in step 1 was correctly specified then there is no signal in the residuals → epsilon will be small

Strong residual confounding → large epsilon → large update Weak residual confounding → small epsilon → small update

## Quick recap

In Step 1, we obtained initial estimates of the expected outcome using super learner. These estimates are optimized to estimate E\[Y\|A,W\] (expected value of the outcome) not the ATE.

We need to update those initial expected outcome estimates using information about the treatment mechanism, so we computed the expected probability of treatment, conditional on confounders, in Step 2.

Step 3, we used quantities from Step 1 and Step 2 to solve an estimating equation for the EIF. This gave us an estimate of epsilon.

## Step 4: update the initial estimates of the expected outcome

We are updating the estimates from step 1 using information from step 2 and 3. The size of epsilon determines the size of the update.

$Q_A_updated = plogis(qlogis(Q_A) + eps*H_A)$

1\. Update the expected outcomes of all observations, given the treatment they actually received and their baseline confounders. `Q_A_updated`

2\. Update the expected outcomes, conditional on baseline confounders and everyone receiving the treatment. (`Q_1_updated`)

3\. Update the expected outcomes, conditional on baseline confounders and no one receiving the treatment. (`Q_0_updated`)

## Step 5: compute the ATE

Mean difference between the updated outcome estimates when everyone is treated vs when no one is treated

ATE = Q_1\_updated - Q_0\_updated

## Keen to learn more?

-   https://www.khstats.com/blog/tmle/tutorial.html
